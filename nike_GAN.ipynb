{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/50][0/47] Loss_D: 1.3835 Loss_G: 1.8286\n",
      "[1/50][0/47] Loss_D: 0.0320 Loss_G: 6.0068\n",
      "[2/50][0/47] Loss_D: 0.0220 Loss_G: 6.5668\n",
      "[3/50][0/47] Loss_D: 0.0141 Loss_G: 5.8555\n",
      "[4/50][0/47] Loss_D: 0.0052 Loss_G: 6.5814\n",
      "[5/50][0/47] Loss_D: 0.0042 Loss_G: 6.7607\n",
      "[6/50][0/47] Loss_D: 0.0043 Loss_G: 6.4646\n",
      "[7/50][0/47] Loss_D: 0.0016 Loss_G: 7.2467\n",
      "[8/50][0/47] Loss_D: 0.0015 Loss_G: 7.3016\n",
      "[9/50][0/47] Loss_D: 0.0010 Loss_G: 7.5703\n",
      "[10/50][0/47] Loss_D: 0.0009 Loss_G: 7.9684\n",
      "[11/50][0/47] Loss_D: 0.0007 Loss_G: 8.0368\n",
      "[12/50][0/47] Loss_D: 0.0007 Loss_G: 8.2007\n",
      "[13/50][0/47] Loss_D: 0.0005 Loss_G: 8.4188\n",
      "[14/50][0/47] Loss_D: 0.0004 Loss_G: 8.6490\n",
      "[15/50][0/47] Loss_D: 0.0004 Loss_G: 8.7449\n",
      "[16/50][0/47] Loss_D: 0.0004 Loss_G: 8.9262\n",
      "[17/50][0/47] Loss_D: 0.0004 Loss_G: 9.0464\n",
      "[18/50][0/47] Loss_D: 0.0005 Loss_G: 9.0817\n",
      "[19/50][0/47] Loss_D: 0.0005 Loss_G: 8.9906\n",
      "[20/50][0/47] Loss_D: 0.0003 Loss_G: 9.2489\n",
      "[21/50][0/47] Loss_D: 0.0006 Loss_G: 9.2106\n",
      "[22/50][0/47] Loss_D: 0.0004 Loss_G: 9.3864\n",
      "[23/50][0/47] Loss_D: 0.0004 Loss_G: 9.1844\n",
      "[24/50][0/47] Loss_D: 0.0003 Loss_G: 9.5128\n",
      "[25/50][0/47] Loss_D: 0.0003 Loss_G: 9.5201\n",
      "[26/50][0/47] Loss_D: 0.0002 Loss_G: 9.8721\n",
      "[27/50][0/47] Loss_D: 0.0002 Loss_G: 9.9252\n",
      "[28/50][0/47] Loss_D: 0.0001 Loss_G: 10.1453\n",
      "[29/50][0/47] Loss_D: 0.0002 Loss_G: 10.0635\n",
      "[30/50][0/47] Loss_D: 0.0001 Loss_G: 10.0753\n",
      "[31/50][0/47] Loss_D: 0.0001 Loss_G: 10.0814\n",
      "[32/50][0/47] Loss_D: 0.0001 Loss_G: 10.3937\n",
      "[33/50][0/47] Loss_D: 0.0001 Loss_G: 10.0287\n",
      "[34/50][0/47] Loss_D: 0.0001 Loss_G: 9.9144\n",
      "[35/50][0/47] Loss_D: 0.0001 Loss_G: 9.8379\n",
      "[36/50][0/47] Loss_D: 0.0001 Loss_G: 9.9905\n",
      "[37/50][0/47] Loss_D: 0.0001 Loss_G: 10.1152\n",
      "[38/50][0/47] Loss_D: 0.0001 Loss_G: 10.1482\n",
      "[39/50][0/47] Loss_D: 0.0001 Loss_G: 10.3072\n",
      "[40/50][0/47] Loss_D: 0.0001 Loss_G: 10.2045\n",
      "[41/50][0/47] Loss_D: 0.0001 Loss_G: 10.4662\n",
      "[42/50][0/47] Loss_D: 0.0000 Loss_G: 10.6493\n",
      "[43/50][0/47] Loss_D: 0.0001 Loss_G: 10.5672\n",
      "[44/50][0/47] Loss_D: 0.0001 Loss_G: 10.4519\n",
      "[45/50][0/47] Loss_D: 0.0001 Loss_G: 10.4433\n",
      "[46/50][0/47] Loss_D: 0.0001 Loss_G: 10.2877\n",
      "[47/50][0/47] Loss_D: 0.0000 Loss_G: 10.7060\n",
      "[48/50][0/47] Loss_D: 0.0000 Loss_G: 11.1943\n",
      "[49/50][0/47] Loss_D: 0.0000 Loss_G: 11.2426\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, utils\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.autograd.variable import Variable\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "lr = 0.0002\n",
    "latent_size = 100\n",
    "epochs = 50\n",
    "\n",
    "class NikeDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_paths = []\n",
    "        for root, dirs, files in os.walk(root_dir):\n",
    "            for file in files:\n",
    "                if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    self.image_paths.append(os.path.join(root, file))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_paths[idx]\n",
    "        image = Image.open(img_name)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image\n",
    "\n",
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = NikeDataset(root_dir='augmented_data', transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_size, 256, kernel_size=4, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 1, kernel_size=4, stride=1, padding=0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Initialize models and optimizers\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        real_images = data.to(device)\n",
    "\n",
    "        # Train Discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        noise = torch.randn(batch_size, latent_size, 1, 1, device=device)\n",
    "        fake_images = generator(noise)\n",
    "\n",
    "        output_real = discriminator(real_images)\n",
    "        output_fake = discriminator(fake_images.detach())\n",
    "\n",
    "        # Binary Cross-Entropy Loss without manual label creation\n",
    "        loss_real = criterion(output_real, torch.ones_like(output_real))\n",
    "        loss_fake = criterion(output_fake, torch.zeros_like(output_fake))\n",
    "\n",
    "        loss_D = loss_real + loss_fake\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Train Generator\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        output_fake = discriminator(fake_images)\n",
    "        \n",
    "        # Binary Cross-Entropy Loss without manual label creation\n",
    "        loss_G = criterion(output_fake, torch.ones_like(output_fake))\n",
    "\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f'\n",
    "                  % (epoch, epochs, i, len(dataloader), loss_D.item(), loss_G.item()))\n",
    "\n",
    "    # Save generated images at the end of each epoch\n",
    "    if epoch % 5 == 0:\n",
    "        with torch.no_grad():\n",
    "            fake = generator(torch.randn(64, latent_size, 1, 1, device=device))\n",
    "        utils.save_image(fake, 'fake_samples_epoch_%03d.png' % epoch, normalize=True)\n",
    "\n",
    "# Save models\n",
    "torch.save(generator.state_dict(), 'generator.pth')\n",
    "torch.save(discriminator.state_dict(), 'discriminator.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Training Accuracy: 99.92%\n",
      "Final Testing Accuracy: 77.91%\n"
     ]
    }
   ],
   "source": [
    "# Print accuracy for the final epoch\n",
    "final_train_accuracy = train_correct[-1] * 100 / len(train_loader.dataset)\n",
    "final_test_accuracy = test_correct[-1] * 100 / len(test_loader.dataset)\n",
    "print(f'\\nFinal Training Accuracy: {final_train_accuracy:.2f}%')\n",
    "print(f'Final Testing Accuracy: {final_test_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
